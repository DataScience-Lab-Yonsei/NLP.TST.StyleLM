{"cells":[{"cell_type":"markdown","metadata":{"id":"NgSkCr6qDl_B"},"source":["# Setting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kBGigJx1DlIv"},"outputs":[],"source":["# 코드나 내용에서 변경있으면 바로 업데이트해서 사용할 수 있게 하는 것\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18331,"status":"ok","timestamp":1665044237409,"user":{"displayName":"CHCHG DSL","userId":"01395977504814535963"},"user_tz":-540},"id":"GfrCbzqfDq3X","outputId":"63565de9-0b92-4f69-a165-ff09dce5b909"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir('/content/drive/MyDrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":67,"status":"ok","timestamp":1665044237412,"user":{"displayName":"CHCHG DSL","userId":"01395977504814535963"},"user_tz":-540},"id":"dsWyxK3MDrsR","outputId":"e798a0fb-c81b-4bd7-9889-28305e752690"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# 현재 활동하고 있는 경로 \n","os.getcwd()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dlmnz0gVDuzv"},"outputs":[],"source":["# 간혹 경로문제가 생길까봐 검색할 때 사용할 기본 경로\n","# sys.path 는 디렉터리의 경로들이 기록된 문자열 리스트\n","#이 리스트에 경로를 추가하면 해당 경로에 있는 파이썬 파일을 import 문으로 불러올 수 있음\n","import sys\n","\n","sys.path.append('/content/drive/MyDrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":61,"status":"ok","timestamp":1665044237419,"user":{"displayName":"CHCHG DSL","userId":"01395977504814535963"},"user_tz":-540},"id":"BdPRsLtM8cyE","outputId":"056ed11e-ea18-478a-8baa-847f1db96c24"},"outputs":[{"data":{"text/plain":["['/content',\n"," '/env/python',\n"," '/usr/lib/python37.zip',\n"," '/usr/lib/python3.7',\n"," '/usr/lib/python3.7/lib-dynload',\n"," '',\n"," '/usr/local/lib/python3.7/dist-packages',\n"," '/usr/lib/python3/dist-packages',\n"," '/usr/local/lib/python3.7/dist-packages/IPython/extensions',\n"," '/root/.ipython',\n"," '/content/drive/MyDrive']"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["sys.path"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60,"status":"ok","timestamp":1665044237422,"user":{"displayName":"CHCHG DSL","userId":"01395977504814535963"},"user_tz":-540},"id":"Ag-zR72-Dz9w","outputId":"e4606bfe-01fc-4835-f744-051dfb16dc71"},"outputs":[{"name":"stdout","output_type":"stream","text":["Thu Oct  6 08:17:16 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   50C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["# 우리가 사용하는 GPU 상태\n","!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8224,"status":"ok","timestamp":1665044245599,"user":{"displayName":"CHCHG DSL","userId":"01395977504814535963"},"user_tz":-540},"id":"NUh57V8DZRah","outputId":"715f1b84-4e47-44b5-cca6-71fb42d21a45"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.22.2-py3-none-any.whl (4.9 MB)\n","\u001b[K     |████████████████████████████████| 4.9 MB 16.5 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 61.3 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting huggingface-hub<1.0,>=0.9.0\n","  Downloading huggingface_hub-0.10.0-py3-none-any.whl (163 kB)\n","\u001b[K     |████████████████████████████████| 163 kB 74.2 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.10.0 tokenizers-0.12.1 transformers-4.22.2\n"]}],"source":["!pip install transformers"]},{"cell_type":"markdown","metadata":{"id":"q2IySiqaD78B"},"source":["# Package\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b-a5fv1oD_N0"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","\n","import json\n","import yaml\n","import random\n","import pickle\n","import numpy as np\n","from tqdm import tqdm\n","import datetime\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kB5mVWM0uyNf"},"outputs":[],"source":["# from utils.data.kogpt_handler import KoGPTDataset"]},{"cell_type":"markdown","metadata":{"id":"tbsE1YlREIGZ"},"source":["# Task..."]},{"cell_type":"markdown","metadata":{"id":"xtjx2G8fZhBd"},"source":["## Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Uxj_vUnZgie"},"outputs":[],"source":["# with open(\"db/scripts/신세계.pkl\", 'rb') as f:\n","#   raw = pickle.load(f)\n","# raw[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"udKgIq-xaRC0"},"outputs":[],"source":["# raw = list(map(lambda x: x.strip(' '), raw))\n","# raw = list(map(lambda x: x.strip('.'), raw))\n","# # raw = list(map(lambda x: x.strip(''), raw))\n","# raw = [i for i in raw if i != '']\n","# raw[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NyJw9gWNBhU1"},"outputs":[],"source":["# with open(\"style_linesci_auto.pkl\", 'rb') as f:\n","with open(\"criminal.txt\", 'r') as f:\n","  raw = f.readlines()\n","raw_two = raw[0].split('   ')\n","raw_two = list(map(lambda x: x[:-3], raw_two))\n","raw_two = [i for i in raw_two if len(i)>3]\n","# raw.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ga8eahYyBt9y"},"outputs":[],"source":["# for k,v in raw.items():\n","#   print(k, len(v))"]},{"cell_type":"markdown","metadata":{"id":"phuiAk-MB0T6"},"source":["\n","\"코미디\" : 0,\n","\"드라마\": 1,\n","\"액션\" : 2,\n","\"범죄\" : 2,\n","\"멜로/로맨스\": 3"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1665044249126,"user":{"displayName":"CHCHG DSL","userId":"01395977504814535963"},"user_tz":-540},"id":"TmVTmGXpBrvb","outputId":"b5a9f834-1f5d-41fb-b0f4-cea55bfa500b"},"outputs":[{"data":{"text/plain":["1255"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# 쓰고 싶은 대본 종류 정하기\n","# raw_two = raw['액션'] + raw['범죄']\n","# raw_two = raw['코미디']\n","len(raw_two)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C7kyeHNrBjci"},"outputs":[],"source":["raw_two[:100]"]},{"cell_type":"markdown","metadata":{"id":"YfwlSqReZ_aF"},"source":["## Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5515,"status":"ok","timestamp":1665044254629,"user":{"displayName":"CHCHG DSL","userId":"01395977504814535963"},"user_tz":-540},"id":"1gSpTGcSaSxe","outputId":"120e639d-6887-43bd-89c9-a9f3cc1a2ebf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2022.6.15)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (1.24.3)\n"]}],"source":["# !pip uninstall requests\n","!pip install requests\n","!pip install urllib3"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4708,"status":"ok","timestamp":1665044259328,"user":{"displayName":"CHCHG DSL","userId":"01395977504814535963"},"user_tz":-540},"id":"0pc8NN6XLNn2","outputId":"715f819d-1d6d-453b-ac42-31a7c2e59388"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 29.1 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.97\n"]}],"source":["!pip install sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vQqaepcv-fC3"},"outputs":[],"source":["# coding=utf-8\n","# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team and Jangwon Park\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\"\"\" Tokenization classes for KoBERT model \"\"\"\n","\n","\n","import logging\n","import os\n","import unicodedata\n","from shutil import copyfile\n","\n","from transformers import PreTrainedTokenizer\n","\n","logger = logging.getLogger(__name__)\n","\n","VOCAB_FILES_NAMES = {\n","    \"vocab_file\": \"tokenizer_78b3253a26.model\",\n","    \"vocab_txt\": \"vocab.txt\",\n","}\n","\n","PRETRAINED_VOCAB_FILES_MAP = {\n","    \"vocab_file\": {\n","        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n","        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n","        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\",\n","    },\n","    \"vocab_txt\": {\n","        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n","        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n","        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\",\n","    },\n","}\n","\n","PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n","    \"monologg/kobert\": 512,\n","    \"monologg/kobert-lm\": 512,\n","    \"monologg/distilkobert\": 512,\n","}\n","\n","PRETRAINED_INIT_CONFIGURATION = {\n","    \"monologg/kobert\": {\"do_lower_case\": False},\n","    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n","    \"monologg/distilkobert\": {\"do_lower_case\": False},\n","}\n","\n","SPIECE_UNDERLINE = \"▁\"\n","\n","# 토큰나이저 클래스\n","class KoBertTokenizer(PreTrainedTokenizer):\n","    \"\"\"\n","    SentencePiece based tokenizer. Peculiarities:\n","        - requires `SentencePiece <https://github.com/google/sentencepiece>`_\n","    \"\"\"\n","\n","    vocab_files_names = VOCAB_FILES_NAMES\n","    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n","    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n","    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n","\n","    def __init__(\n","        self,\n","        vocab_file,\n","        vocab_txt,\n","        do_lower_case=False,\n","        remove_space=True,\n","        keep_accents=False,\n","        unk_token=\"[UNK]\",\n","        sep_token=\"[SEP]\",\n","        pad_token=\"[PAD]\",\n","        cls_token=\"[CLS]\",\n","        mask_token=\"[MASK]\",\n","        **kwargs,\n","    ):\n","        super().__init__(\n","            unk_token=unk_token,\n","            sep_token=sep_token,\n","            pad_token=pad_token,\n","            cls_token=cls_token,\n","            mask_token=mask_token,\n","            **kwargs,\n","        )\n","\n","        # Build vocab\n","        self.token2idx = dict()\n","        self.idx2token = []\n","        with open(vocab_txt, \"r\", encoding=\"utf-8\") as f:\n","            for idx, token in enumerate(f):\n","                token = token.strip()\n","                self.token2idx[token] = idx\n","                self.idx2token.append(token)\n","\n","        try:\n","            import sentencepiece as spm\n","        except ImportError:\n","            logger.warning(\n","                \"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n","                \"pip install sentencepiece\"\n","            )\n","\n","        self.do_lower_case = do_lower_case\n","        self.remove_space = remove_space\n","        self.keep_accents = keep_accents\n","        self.vocab_file = vocab_file\n","        self.vocab_txt = vocab_txt\n","\n","        self.sp_model = spm.SentencePieceProcessor()\n","        self.sp_model.Load(vocab_file)\n","\n","    @property\n","    def vocab_size(self):\n","        return len(self.idx2token)\n","\n","    def get_vocab(self):\n","        return dict(self.token2idx, **self.added_tokens_encoder)\n","\n","    def __getstate__(self):\n","        state = self.__dict__.copy()\n","        state[\"sp_model\"] = None\n","        return state\n","\n","    def __setstate__(self, d):\n","        self.__dict__ = d\n","        try:\n","            import sentencepiece as spm\n","        except ImportError:\n","            logger.warning(\n","                \"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n","                \"pip install sentencepiece\"\n","            )\n","        self.sp_model = spm.SentencePieceProcessor()\n","        self.sp_model.Load(self.vocab_file)\n","\n","    def preprocess_text(self, inputs):\n","        if self.remove_space:\n","            outputs = \" \".join(inputs.strip().split())\n","        else:\n","            outputs = inputs\n","        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n","\n","        if not self.keep_accents:\n","            outputs = unicodedata.normalize(\"NFKD\", outputs)\n","            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n","        if self.do_lower_case:\n","            outputs = outputs.lower()\n","\n","        return outputs\n","\n","    def _tokenize(self, text):\n","        \"\"\"Tokenize a string.\"\"\"\n","        text = self.preprocess_text(text)\n","        pieces = self.sp_model.encode(text, out_type=str)\n","        new_pieces = []\n","        for piece in pieces:\n","            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n","                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n","                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n","                    if len(cur_pieces[0]) == 1:\n","                        cur_pieces = cur_pieces[1:]\n","                    else:\n","                        cur_pieces[0] = cur_pieces[0][1:]\n","                cur_pieces.append(piece[-1])\n","                new_pieces.extend(cur_pieces)\n","            else:\n","                new_pieces.append(piece)\n","\n","        return new_pieces\n","\n","    def _convert_token_to_id(self, token):\n","        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n","        return self.token2idx.get(token, self.token2idx[self.unk_token])\n","\n","    def _convert_id_to_token(self, index):\n","        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n","        return self.idx2token[index]\n","\n","    def convert_tokens_to_string(self, tokens):\n","        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n","        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n","        return out_string\n","\n","    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n","        \"\"\"\n","        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n","        by concatenating and adding special tokens.\n","        A KoBERT sequence has the following format:\n","            single sequence: [CLS] X [SEP]\n","            pair of sequences: [CLS] A [SEP] B [SEP]\n","        \"\"\"\n","        if token_ids_1 is None:\n","            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n","        cls = [self.cls_token_id]\n","        sep = [self.sep_token_id]\n","        return cls + token_ids_0 + sep + token_ids_1 + sep\n","\n","    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n","        \"\"\"\n","        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n","        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n","        Args:\n","            token_ids_0: list of ids (must not contain special tokens)\n","            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n","                for sequence pairs\n","            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n","                special tokens for the model\n","        Returns:\n","            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n","        \"\"\"\n","\n","        if already_has_special_tokens:\n","            if token_ids_1 is not None:\n","                raise ValueError(\n","                    \"You should not supply a second sequence if the provided sequence of \"\n","                    \"ids is already formated with special tokens for the model.\"\n","                )\n","            return list(\n","                map(\n","                    lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0,\n","                    token_ids_0,\n","                )\n","            )\n","\n","        if token_ids_1 is not None:\n","            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n","        return [1] + ([0] * len(token_ids_0)) + [1]\n","\n","    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n","        \"\"\"\n","        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n","        A KoBERT sequence pair mask has the following format:\n","        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n","        | first sequence    | second sequence\n","        if token_ids_1 is None, only returns the first portion of the mask (0's).\n","        \"\"\"\n","        sep = [self.sep_token_id]\n","        cls = [self.cls_token_id]\n","        if token_ids_1 is None:\n","            return len(cls + token_ids_0 + sep) * [0]\n","        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n","\n","    def save_vocabulary(self, save_directory, filename_prefix):\n","        \"\"\"Save the sentencepiece vocabulary (copy original file) and special tokens file\n","        to a directory.\n","        \"\"\"\n","        if not os.path.isdir(save_directory):\n","            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n","            return\n","\n","        # 1. Save sentencepiece model\n","        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n","\n","        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n","            copyfile(self.vocab_file, out_vocab_model)\n","\n","        # 2. Save vocab.txt\n","        index = 0\n","        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n","        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n","            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n","                if index != token_index:\n","                    logger.warning(\n","                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n","                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n","                    )\n","                    index = token_index\n","                writer.write(token + \"\\n\")\n","                index += 1\n","\n","        return out_vocab_model, out_vocab_txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J4cU8PtLcNBl"},"outputs":[],"source":["from transformers import PreTrainedTokenizerFast\n","from transformers import PreTrainedTokenizer\n","\n","from transformers import AutoModel, AutoTokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["700df92bde1840938a4dc6879397c07d","f3c122343e2f49abbdbca22f2dd424cb","3d687c6d9b8c49338109d55eeef70518","ba9c4b3d78e54d4c943a51dccc3eecf0","a6fe3fd0cc1c4198ba3e77e5ec52d1ed","275a852cb1984eac927778bf47ee5ac2","d0ac2c4fdfa848309905cb00c01ffe9d","714e545d98fe4060b167b46ba19a0599","698ab21f4afd4490b55b54bd794d3652","f918ed5af5e446e9b5883815169d3184","6605c07e889a4f43ae9eeb5ffefda3ce","987a486c31ec4655b1a93856a59f92a2","47a940495ebe4ea38bb29092cca155ff","b702fefd7ce34cd4bf51ff8317148259","938877cb177d4e1aa9252e65870dc654","e64b028eb3574458a0943246d1699718","927a24893c3a4bbd9a4b3785189ab261","5c668eda0a114dd982d535c365281ce6","dd836cf6ac984e82aa592f7f8025be6a","1b99c16bff30466dbe36905fce83e84d","41c831f1819042c99292bacf41d91e40","c904790f16884b1c8edcaab61e570d52","014e23e25f604d75883a585fb49f02d3","0029cb3dff6643a1b12aebc091ab9542","194e02de56214d5db0acd08f51dfab0a","99c1f88b17864bb1a30a0a37cfc8c399","c05cc1eca0ec426ea8dc95687ce76753","ea95640963074b109996160f05439ed5","7bf59e8fc38c4d3b8cf2ee83e65da0eb","a12bad86350245ab8b7d8c37eaedb613","d745ebd64ef04be3aa949a4483442ec0","07aa6200c6704117a6f68ef0070bbcfc","6f5adda1233b4f94bb2ce962d734861e","5f439aa95b1146e380989ce28d6fffc3","eb9ad837d762489185ecac1528252656","1a796958a51846a49e78a2080931688f","8e80f1af44894fa3a234e48cabdbf3a1","0767249623f04e51bc0f08ca10f32162","ad4d523ca6b74d32ab46f86f29a447f7","ca09064a38ba46449fc04f8edf0704e0","1c452e33546249b780bfba6c082ab8fb","611ad37aea14437eb3e5df7ccfa29408","571aba16f1af44bcaef87b73b8b0fbd0","3f4b798ff3dc49a09cd7f173bcefe32a"]},"executionInfo":{"elapsed":115,"status":"ok","timestamp":1665044259334,"user":{"displayName":"CHCHG DSL","userId":"01395977504814535963"},"user_tz":-540},"id":"-rrEiGUM7fNv","outputId":"75549cd0-a9dd-4b05-86fa-12202c4c78c7"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"700df92bde1840938a4dc6879397c07d","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/289 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"987a486c31ec4655b1a93856a59f92a2","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/425 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"014e23e25f604d75883a585fb49f02d3","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/495k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5f439aa95b1146e380989ce28d6fffc3","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/125 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PvfH_9o9cLKI"},"outputs":[],"source":["# tokenizer = PreTrainedTokenizerFast.from_pretrained(\"klue/bert-base\", \n","#                                                     bos_token='</s>', \n","#                                                     eos_token='</s>', \n","#                                                     unk_token='<unk>',\n","#                                                     pad_token='<pad>', \n","#                                                     mask_token='<mask>')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52,"status":"ok","timestamp":1665044259341,"user":{"displayName":"CHCHG DSL","userId":"01395977504814535963"},"user_tz":-540},"id":"vxVzcIBz6Ei-","outputId":"a1fd9268-6b04-45df-8567-7bcca73a89cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["[UNK] 1\n"]}],"source":["for k, v in tokenizer.vocab.items():\n","  if v==1:\n","    print(k, v)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52,"status":"ok","timestamp":1665044259343,"user":{"displayName":"CHCHG DSL","userId":"01395977504814535963"},"user_tz":-540},"id":"9ztO3txPcSXt","outputId":"09345f95-de57-4f71-d0d3-ed448560e798"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenize 전 :   사람들이 왜 도박을 하냐구요? 글쎄요.. 대부분 스릴 때문에 한다고 하잖아요? 따느냐 잃느냐. 근데 정말 그럴까요? 어느 날 고니가 얘기해 주더라고요. 사람들이 도박을 왜 하는지\n","Tokenize 후 :  ['사람', '##들이', '왜', '도박', '##을', '하', '##냐', '##구요', '?', '글쎄요', '.', '.', '대부분', '스릴', '때문', '##에', '한다고', '하', '##잖아요', '?', '따', '##느', '##냐', '잃', '##느', '##냐', '.', '근데', '정말', '그럴까요', '?', '어느', '날', '고', '##니', '##가', '얘기', '##해', '주', '##더라', '##고', '##요', '.', '사람', '##들이', '도박', '##을', '왜', '하', '##는지']\n","Tokenize 후 :  [2, 3611, 7285, 1460, 8902, 2069, 1889, 2529, 5515, 35, 15817, 18, 18, 4057, 15314, 3624, 2170, 4034, 1889, 20754, 35, 897, 2922, 2529, 1509, 2922, 2529, 18, 4224, 3944, 31255, 35, 3875, 721, 594, 2209, 2116, 3880, 2097, 1564, 23677, 2088, 2182, 18, 3611, 7285, 8902, 2069, 1460, 1889, 18246, 3]\n"]}],"source":["idx = 8\n","print(\"Tokenize 전 : \", raw_two[idx])\n","print(\"Tokenize 후 : \", tokenizer.tokenize(raw_two[idx]))\n","print(\"Tokenize 후 : \", tokenizer.encode(raw_two[idx]))"]},{"cell_type":"markdown","metadata":{"id":"a-2zS4miczy1"},"source":["## Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["2ffe40f696a34ddd94c47f683b557067","a2f0192606d443089212c17bbe4234d1","9e7a5461b38e44ceb50264df6d888f1e","27a00c9a445e4941ae967940c8565194","15d2299a52784009be9b55c0609a8948","0f1ac225c12248ce8abe681671b0c2bd","39d28db54f934c20b5ab64e1e4885e77","5c307d72d6ee42ba9277e633a40f9183","e19e8215b7984a50b71f37daa400df05","3951984ff1e24848b3cdccfef12ffc93","87c0384dda67414588195ff266b593bb"]},"executionInfo":{"elapsed":14257,"status":"ok","timestamp":1665044273551,"user":{"displayName":"CHCHG DSL","userId":"01395977504814535963"},"user_tz":-540},"id":"flpWZ-twEAU6","outputId":"39c73e89-64ef-49aa-8361-d3a91d1e30e7"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2ffe40f696a34ddd94c47f683b557067","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/445M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at klue/bert-base were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertLMHeadModel were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import EncoderDecoderModel\n","bert2bert = EncoderDecoderModel.from_encoder_decoder_pretrained(\"klue/bert-base\", \n","                                                                \"klue/bert-base\")"]},{"cell_type":"markdown","metadata":{"id":"VInLJPNUvSeC"},"source":["## Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5423,"status":"ok","timestamp":1665044278918,"user":{"displayName":"CHCHG DSL","userId":"01395977504814535963"},"user_tz":-540},"id":"Qd7qpgo3DbJq","outputId":"a986432e-f7cd-4699-80f7-df1240011c6e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.5.2-py3-none-any.whl (432 kB)\n","\u001b[K     |████████████████████████████████| 432 kB 18.8 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 67.7 MB/s \n","\u001b[?25hRequirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.8.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.10.0)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n","\u001b[K     |████████████████████████████████| 115 kB 67.4 MB/s \n","\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 72.7 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.2.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: urllib3, xxhash, responses, multiprocess, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed datasets-2.5.2 multiprocess-0.70.13 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0\n"]},{"data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["urllib3"]}}},"metadata":{},"output_type":"display_data"}],"source":["# 이걸 맨 마지막에 해야함\n","# \n","!pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2028,"status":"ok","timestamp":1665044280930,"user":{"displayName":"CHCHG DSL","userId":"01395977504814535963"},"user_tz":-540},"id":"-MK6V-phKMk5","outputId":"c9c76e51-1c68-4c32-b319-791396ca1aa2"},"outputs":[{"name":"stderr","output_type":"stream","text":["[autoreload of urllib3.connectionpool failed: Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/extensions/autoreload.py\", line 245, in check\n","    superreload(m, reload, self.old_objects)\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n","    module = reload(module)\n","  File \"/usr/lib/python3.7/imp.py\", line 314, in reload\n","    return importlib.reload(module)\n","  File \"/usr/lib/python3.7/importlib/__init__.py\", line 169, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n","  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n","  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n","  File \"/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\", line 46, in <module>\n","    from .util.url import (\n","ImportError: cannot import name '_normalize_host' from 'urllib3.util.url' (/usr/local/lib/python3.7/dist-packages/urllib3/util/url.py)\n","]\n","[autoreload of urllib3.packages.six failed: Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/extensions/autoreload.py\", line 245, in check\n","    superreload(m, reload, self.old_objects)\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n","    update_generic(old_obj, new_obj)\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n","    update(a, b)\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/extensions/autoreload.py\", line 287, in update_class\n","    old_obj = getattr(old, key)\n","  File \"/usr/local/lib/python3.7/dist-packages/urllib3/packages/six.py\", line 92, in __get__\n","    setattr(obj, self.name, result)  # Invokes __set__.\n","AttributeError: 'NoneType' object has no attribute 'cStringIO'\n","]\n","[autoreload of urllib3.util failed: Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/extensions/autoreload.py\", line 245, in check\n","    superreload(m, reload, self.old_objects)\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n","    module = reload(module)\n","  File \"/usr/lib/python3.7/imp.py\", line 314, in reload\n","    return importlib.reload(module)\n","  File \"/usr/lib/python3.7/importlib/__init__.py\", line 169, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n","  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n","  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n","  File \"/usr/local/lib/python3.7/dist-packages/urllib3/util/__init__.py\", line 7, in <module>\n","    from .ssl_ import (\n","ImportError: cannot import name 'PROTOCOL_TLS' from 'urllib3.util.ssl_' (/usr/local/lib/python3.7/dist-packages/urllib3/util/ssl_.py)\n","]\n","[autoreload of urllib3.util.ssl_ failed: Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/extensions/autoreload.py\", line 245, in check\n","    superreload(m, reload, self.old_objects)\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n","    module = reload(module)\n","  File \"/usr/lib/python3.7/imp.py\", line 314, in reload\n","    return importlib.reload(module)\n","  File \"/usr/lib/python3.7/importlib/__init__.py\", line 169, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n","  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n","  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n","  File \"/usr/local/lib/python3.7/dist-packages/urllib3/util/ssl_.py\", line 11, in <module>\n","    from .url import IPV4_RE, BRACELESS_IPV6_ADDRZ_RE\n","ImportError: cannot import name 'IPV4_RE' from 'urllib3.util.url' (/usr/local/lib/python3.7/dist-packages/urllib3/util/url.py)\n","]\n"]}],"source":["import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XeX0FvAgDema"},"outputs":[],"source":["from datasets import Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mSZELAe9DjzV"},"outputs":[],"source":["raw_ds = Dataset.from_dict({\"text\":raw_two# list\n","                            \n","                            })"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HXX2hMD9DqHk"},"outputs":[],"source":["enc_max_len = 32\n","dec_max_len = 32\n","\n","def data_to_input(batch):\n","    # tokenize the inputs and labels\n","    inputs = tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=enc_max_len)\n","    # [나 , 밥, -을, 먹-,었-,다]\n","    # 노이즈 예씨\n","    # [나 , MASK, -을, 먹-,었-,다]\n","    # [나 , 치킨, -을, 먹-,었-,다]\n","    # (in)문장 --> noise --> 문장(out)\n","    \n","    # StyleLM\n","    # Noise\n","    p_drop = 0.15*0.8\n","    idx = np.random.choice([0, 1], size= len(inputs), p=[p_drop, 1-p_drop])\n","    for i in idx:\n","      if i == 0:\n","        inputs[i] = 32003 # 32003번은 MASK 토큰의 ID --> BLANK for klue\n","\n","    outputs = tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=dec_max_len)\n","\n","    batch[\"input_ids\"] = [i[1:] for i in inputs.input_ids]\n","    batch[\"attention_mask\"] = [i[1:] for i in inputs.attention_mask]\n","    batch[\"decoder_input_ids\"] = [i[1:] for i in inputs.input_ids]\n","    batch[\"decoder_attention_mask\"] = [i[1:] for i in outputs.attention_mask]\n","    batch[\"labels\"] = [i[1:] for i in outputs.input_ids]\n","#     batch[\"input_ids\"] = torch.LongTensor(inputs.input_ids).to(\"cuda\")\n","#     batch[\"attention_mask\"] = torch.LongTensor(inputs.attention_mask).to(\"cuda\")\n","#     batch[\"decoder_input_ids\"] = torch.LongTensor(outputs.input_ids).to(\"cuda\")\n","#     batch[\"decoder_attention_mask\"] = torch.LongTensor(outputs.attention_mask).to(\"cuda\")\n","#     batch[\"labels\"] = torch.LongTensor(outputs.input_ids).to(\"cuda\")\n","\n","    # because BERT automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`. \n","    # We have to make sure that the PAD token is ignored\n","    batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n","    return batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7rJumhIQDs7r"},"outputs":[],"source":["batch_size = 6"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["9d0020dec15f41118735b0eaf066ac87","d98a7c916e9f4886af9495a1e897d1c8","bffa46cb140244769aba92514266cc27","0cf73f69504949b3b54234993bf6ac58","5e1e3299147f4590bd390e3082f67f6c","f961b7e26c3742258c44298780f98b0a","952630f8838b43e2b2e09e3ac29ee5cf","1735af002c9445bc853ca445a64bdf93","8b719707b2e041d68e2412e7e16ad0b7","666a9aba877b491b93834995fc665bb6","69a097af95054d00a0d8c1fd5621324f"]},"executionInfo":{"elapsed":391,"status":"ok","timestamp":1665044283022,"user":{"displayName":"CHCHG DSL","userId":"01395977504814535963"},"user_tz":-540},"id":"BAmStIMlDuoN","outputId":"508ff3b5-0fe2-4110-b58c-adc4988868e9"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d0020dec15f41118735b0eaf066ac87","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/210 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["train_data = raw_ds.map(\n","    data_to_input, \n","    batched=True, \n","    batch_size=batch_size, \n","    remove_columns=[\"text\"]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vQZoAY7bD6m4"},"outputs":[],"source":["# 모델이 쓰고 있는 torch의 텐서 형태로 정리함!\n","train_data.set_format(\n","    type=\"torch\", columns=['input_ids', 'attention_mask', 'decoder_input_ids', 'decoder_attention_mask', 'labels'],\n",") "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SojHDWCND8PI"},"outputs":[],"source":["train_data = train_data.select(range(100, len(train_data)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zl5xaR7UD9FA"},"outputs":[],"source":["val_data = train_data.select(range(100))"]},{"cell_type":"markdown","metadata":{"id":"PRQrY-uywMYW"},"source":["# Train"]},{"cell_type":"markdown","metadata":{"id":"vPKOhlxu5iAJ"},"source":["## Utils"]},{"cell_type":"markdown","metadata":{"id":"FLu-VxkSxh7y"},"source":["## Implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rJ2O2USkGdrt"},"outputs":[],"source":["from transformers import Trainer, TrainingArguments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XXmlG6orGfU4"},"outputs":[],"source":["# 엔지니어링! 몇번이나 백업할것인지, 내부 작업, beta값(Adam) ..\n","training_args = TrainingArguments(\n","    num_train_epochs=20,\n","    evaluation_strategy=\"steps\",\n","    prediction_loss_only=True,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    # fp16=True, \n","    # 저장할 디렉토리\n","    # 각자 자기 디렉토리 사용하기!!\n","    output_dir=\"./ckpt/YJ\",\n","\n","    # train loss 기록 주기\n","    logging_steps=100,\n","\n","    # 파일 백업 저장 주기\n","    save_steps=500,\n","\n","    # 평가주기\n","    eval_steps=100,\n","\n","    learning_rate=8e-6,\n","    load_best_model_at_end=True,\n","    \n","    warmup_steps=500,\n","    save_total_limit=3,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oCM9pjswtmsf"},"outputs":[],"source":["from torch import nn\n","class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.get(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.get(\"logits\")\n","\n","        loss_fct = nn.CrossEntropyLoss(reduction=\"mean\")\n","        loss = loss_fct(logits.view(-1, self.model.config.decoder.vocab_size), labels.view(-1))\n","        return (loss, outputs) if return_outputs else loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PNEfch18Go7o"},"outputs":[],"source":["import torch\n","\n","optims = torch.optim.SGD"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MthKUqW8Ggko"},"outputs":[],"source":["# instantiate trainer\n","trainer = CustomTrainer(\n","    model=bert2bert,\n","    tokenizer=tokenizer,\n","    args=training_args,\n","    train_dataset=train_data,\n","    eval_dataset=val_data,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"1GCzXQTVGig8","outputId":"0efc3b83-7085-4988-d15b-90b9f94067be"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1155\n","  Num Epochs = 20\n","  Instantaneous batch size per device = 6\n","  Total train batch size (w. parallel, distributed & accumulation) = 6\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3860\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3860' max='3860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3860/3860 18:56, Epoch 20/20]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>3.541600</td>\n","      <td>1.140352</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>1.433600</td>\n","      <td>0.171743</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.471400</td>\n","      <td>0.014938</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.186900</td>\n","      <td>0.005071</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.093100</td>\n","      <td>0.003056</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.053100</td>\n","      <td>0.001674</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.027800</td>\n","      <td>0.001616</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.022900</td>\n","      <td>0.001320</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.014400</td>\n","      <td>0.000724</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.012600</td>\n","      <td>0.000734</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.009200</td>\n","      <td>0.000590</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.007500</td>\n","      <td>0.000295</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.005500</td>\n","      <td>0.000308</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.005100</td>\n","      <td>0.000335</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.003500</td>\n","      <td>0.000257</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.003200</td>\n","      <td>0.000187</td>\n","    </tr>\n","    <tr>\n","      <td>1700</td>\n","      <td>0.002400</td>\n","      <td>0.000119</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>0.003000</td>\n","      <td>0.000116</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.002400</td>\n","      <td>0.000106</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.001900</td>\n","      <td>0.000091</td>\n","    </tr>\n","    <tr>\n","      <td>2100</td>\n","      <td>0.002100</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>2200</td>\n","      <td>0.001800</td>\n","      <td>0.000093</td>\n","    </tr>\n","    <tr>\n","      <td>2300</td>\n","      <td>0.001900</td>\n","      <td>0.000083</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>0.001600</td>\n","      <td>0.000087</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.001500</td>\n","      <td>0.000075</td>\n","    </tr>\n","    <tr>\n","      <td>2600</td>\n","      <td>0.001500</td>\n","      <td>0.000076</td>\n","    </tr>\n","    <tr>\n","      <td>2700</td>\n","      <td>0.001700</td>\n","      <td>0.000090</td>\n","    </tr>\n","    <tr>\n","      <td>2800</td>\n","      <td>0.001500</td>\n","      <td>0.000081</td>\n","    </tr>\n","    <tr>\n","      <td>2900</td>\n","      <td>0.001300</td>\n","      <td>0.000091</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.001100</td>\n","      <td>0.000076</td>\n","    </tr>\n","    <tr>\n","      <td>3100</td>\n","      <td>0.001500</td>\n","      <td>0.000076</td>\n","    </tr>\n","    <tr>\n","      <td>3200</td>\n","      <td>0.001200</td>\n","      <td>0.000077</td>\n","    </tr>\n","    <tr>\n","      <td>3300</td>\n","      <td>0.001000</td>\n","      <td>0.000077</td>\n","    </tr>\n","    <tr>\n","      <td>3400</td>\n","      <td>0.001200</td>\n","      <td>0.000063</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.001000</td>\n","      <td>0.000063</td>\n","    </tr>\n","    <tr>\n","      <td>3600</td>\n","      <td>0.001000</td>\n","      <td>0.000060</td>\n","    </tr>\n","    <tr>\n","      <td>3700</td>\n","      <td>0.001000</td>\n","      <td>0.000058</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000900</td>\n","      <td>0.000059</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","Saving model checkpoint to ./ckpt/YJ/checkpoint-500\n","Configuration saved in ./ckpt/YJ/checkpoint-500/config.json\n","Model weights saved in ./ckpt/YJ/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in ./ckpt/YJ/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in ./ckpt/YJ/checkpoint-500/special_tokens_map.json\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","Saving model checkpoint to ./ckpt/YJ/checkpoint-1000\n","Configuration saved in ./ckpt/YJ/checkpoint-1000/config.json\n","Model weights saved in ./ckpt/YJ/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in ./ckpt/YJ/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in ./ckpt/YJ/checkpoint-1000/special_tokens_map.json\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","Saving model checkpoint to ./ckpt/YJ/checkpoint-1500\n","Configuration saved in ./ckpt/YJ/checkpoint-1500/config.json\n","Model weights saved in ./ckpt/YJ/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in ./ckpt/YJ/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in ./ckpt/YJ/checkpoint-1500/special_tokens_map.json\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","Saving model checkpoint to ./ckpt/YJ/checkpoint-2000\n","Configuration saved in ./ckpt/YJ/checkpoint-2000/config.json\n","Model weights saved in ./ckpt/YJ/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in ./ckpt/YJ/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in ./ckpt/YJ/checkpoint-2000/special_tokens_map.json\n","Deleting older checkpoint [ckpt/YJ/checkpoint-500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","Saving model checkpoint to ./ckpt/YJ/checkpoint-2500\n","Configuration saved in ./ckpt/YJ/checkpoint-2500/config.json\n","Model weights saved in ./ckpt/YJ/checkpoint-2500/pytorch_model.bin\n","tokenizer config file saved in ./ckpt/YJ/checkpoint-2500/tokenizer_config.json\n","Special tokens file saved in ./ckpt/YJ/checkpoint-2500/special_tokens_map.json\n","Deleting older checkpoint [ckpt/YJ/checkpoint-1000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","Saving model checkpoint to ./ckpt/YJ/checkpoint-3000\n","Configuration saved in ./ckpt/YJ/checkpoint-3000/config.json\n","Model weights saved in ./ckpt/YJ/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in ./ckpt/YJ/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in ./ckpt/YJ/checkpoint-3000/special_tokens_map.json\n","Deleting older checkpoint [ckpt/YJ/checkpoint-1500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","Saving model checkpoint to ./ckpt/YJ/checkpoint-3500\n","Configuration saved in ./ckpt/YJ/checkpoint-3500/config.json\n","Model weights saved in ./ckpt/YJ/checkpoint-3500/pytorch_model.bin\n","tokenizer config file saved in ./ckpt/YJ/checkpoint-3500/tokenizer_config.json\n","Special tokens file saved in ./ckpt/YJ/checkpoint-3500/special_tokens_map.json\n","Deleting older checkpoint [ckpt/YJ/checkpoint-2000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n","  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 6\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ./ckpt/YJ/checkpoint-3500 (score: 6.29241912974976e-05).\n"]},{"data":{"text/plain":["TrainOutput(global_step=3860, training_loss=0.1535329830210289, metrics={'train_runtime': 1141.1502, 'train_samples_per_second': 20.243, 'train_steps_per_second': 3.383, 'total_flos': 858003384268800.0, 'train_loss': 0.1535329830210289, 'epoch': 20.0})"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["NgSkCr6qDl_B","q2IySiqaD78B","xtjx2G8fZhBd","VInLJPNUvSeC"],"toc_visible":true,"provenance":[],"authorship_tag":"ABX9TyNqNBlej9TO5ZYeSnF6Nu7g"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0029cb3dff6643a1b12aebc091ab9542":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea95640963074b109996160f05439ed5","placeholder":"​","style":"IPY_MODEL_7bf59e8fc38c4d3b8cf2ee83e65da0eb","value":"Downloading: 100%"}},"014e23e25f604d75883a585fb49f02d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0029cb3dff6643a1b12aebc091ab9542","IPY_MODEL_194e02de56214d5db0acd08f51dfab0a","IPY_MODEL_99c1f88b17864bb1a30a0a37cfc8c399"],"layout":"IPY_MODEL_c05cc1eca0ec426ea8dc95687ce76753"}},"0767249623f04e51bc0f08ca10f32162":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07aa6200c6704117a6f68ef0070bbcfc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0cf73f69504949b3b54234993bf6ac58":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_666a9aba877b491b93834995fc665bb6","placeholder":"​","style":"IPY_MODEL_69a097af95054d00a0d8c1fd5621324f","value":" 210/210 [00:00&lt;00:00, 313.23ba/s]"}},"0f1ac225c12248ce8abe681671b0c2bd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15d2299a52784009be9b55c0609a8948":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1735af002c9445bc853ca445a64bdf93":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"194e02de56214d5db0acd08f51dfab0a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a12bad86350245ab8b7d8c37eaedb613","max":494860,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d745ebd64ef04be3aa949a4483442ec0","value":494860}},"1a796958a51846a49e78a2080931688f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c452e33546249b780bfba6c082ab8fb","max":125,"min":0,"orientation":"horizontal","style":"IPY_MODEL_611ad37aea14437eb3e5df7ccfa29408","value":125}},"1b99c16bff30466dbe36905fce83e84d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1c452e33546249b780bfba6c082ab8fb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"275a852cb1984eac927778bf47ee5ac2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27a00c9a445e4941ae967940c8565194":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3951984ff1e24848b3cdccfef12ffc93","placeholder":"​","style":"IPY_MODEL_87c0384dda67414588195ff266b593bb","value":" 445M/445M [00:07&lt;00:00, 63.9MB/s]"}},"2ffe40f696a34ddd94c47f683b557067":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a2f0192606d443089212c17bbe4234d1","IPY_MODEL_9e7a5461b38e44ceb50264df6d888f1e","IPY_MODEL_27a00c9a445e4941ae967940c8565194"],"layout":"IPY_MODEL_15d2299a52784009be9b55c0609a8948"}},"3951984ff1e24848b3cdccfef12ffc93":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39d28db54f934c20b5ab64e1e4885e77":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d687c6d9b8c49338109d55eeef70518":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_714e545d98fe4060b167b46ba19a0599","max":289,"min":0,"orientation":"horizontal","style":"IPY_MODEL_698ab21f4afd4490b55b54bd794d3652","value":289}},"3f4b798ff3dc49a09cd7f173bcefe32a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41c831f1819042c99292bacf41d91e40":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47a940495ebe4ea38bb29092cca155ff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_927a24893c3a4bbd9a4b3785189ab261","placeholder":"​","style":"IPY_MODEL_5c668eda0a114dd982d535c365281ce6","value":"Downloading: 100%"}},"571aba16f1af44bcaef87b73b8b0fbd0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c307d72d6ee42ba9277e633a40f9183":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c668eda0a114dd982d535c365281ce6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5e1e3299147f4590bd390e3082f67f6c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f439aa95b1146e380989ce28d6fffc3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eb9ad837d762489185ecac1528252656","IPY_MODEL_1a796958a51846a49e78a2080931688f","IPY_MODEL_8e80f1af44894fa3a234e48cabdbf3a1"],"layout":"IPY_MODEL_0767249623f04e51bc0f08ca10f32162"}},"611ad37aea14437eb3e5df7ccfa29408":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6605c07e889a4f43ae9eeb5ffefda3ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"666a9aba877b491b93834995fc665bb6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"698ab21f4afd4490b55b54bd794d3652":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"69a097af95054d00a0d8c1fd5621324f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f5adda1233b4f94bb2ce962d734861e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"700df92bde1840938a4dc6879397c07d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f3c122343e2f49abbdbca22f2dd424cb","IPY_MODEL_3d687c6d9b8c49338109d55eeef70518","IPY_MODEL_ba9c4b3d78e54d4c943a51dccc3eecf0"],"layout":"IPY_MODEL_a6fe3fd0cc1c4198ba3e77e5ec52d1ed"}},"714e545d98fe4060b167b46ba19a0599":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7bf59e8fc38c4d3b8cf2ee83e65da0eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"87c0384dda67414588195ff266b593bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8b719707b2e041d68e2412e7e16ad0b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8e80f1af44894fa3a234e48cabdbf3a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_571aba16f1af44bcaef87b73b8b0fbd0","placeholder":"​","style":"IPY_MODEL_3f4b798ff3dc49a09cd7f173bcefe32a","value":" 125/125 [00:00&lt;00:00, 4.07kB/s]"}},"927a24893c3a4bbd9a4b3785189ab261":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"938877cb177d4e1aa9252e65870dc654":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_41c831f1819042c99292bacf41d91e40","placeholder":"​","style":"IPY_MODEL_c904790f16884b1c8edcaab61e570d52","value":" 425/425 [00:00&lt;00:00, 5.88kB/s]"}},"952630f8838b43e2b2e09e3ac29ee5cf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"987a486c31ec4655b1a93856a59f92a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_47a940495ebe4ea38bb29092cca155ff","IPY_MODEL_b702fefd7ce34cd4bf51ff8317148259","IPY_MODEL_938877cb177d4e1aa9252e65870dc654"],"layout":"IPY_MODEL_e64b028eb3574458a0943246d1699718"}},"99c1f88b17864bb1a30a0a37cfc8c399":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_07aa6200c6704117a6f68ef0070bbcfc","placeholder":"​","style":"IPY_MODEL_6f5adda1233b4f94bb2ce962d734861e","value":" 495k/495k [00:00&lt;00:00, 8.94MB/s]"}},"9d0020dec15f41118735b0eaf066ac87":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d98a7c916e9f4886af9495a1e897d1c8","IPY_MODEL_bffa46cb140244769aba92514266cc27","IPY_MODEL_0cf73f69504949b3b54234993bf6ac58"],"layout":"IPY_MODEL_5e1e3299147f4590bd390e3082f67f6c"}},"9e7a5461b38e44ceb50264df6d888f1e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c307d72d6ee42ba9277e633a40f9183","max":445025130,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e19e8215b7984a50b71f37daa400df05","value":445025130}},"a12bad86350245ab8b7d8c37eaedb613":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2f0192606d443089212c17bbe4234d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f1ac225c12248ce8abe681671b0c2bd","placeholder":"​","style":"IPY_MODEL_39d28db54f934c20b5ab64e1e4885e77","value":"Downloading: 100%"}},"a6fe3fd0cc1c4198ba3e77e5ec52d1ed":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad4d523ca6b74d32ab46f86f29a447f7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b702fefd7ce34cd4bf51ff8317148259":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd836cf6ac984e82aa592f7f8025be6a","max":425,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1b99c16bff30466dbe36905fce83e84d","value":425}},"ba9c4b3d78e54d4c943a51dccc3eecf0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f918ed5af5e446e9b5883815169d3184","placeholder":"​","style":"IPY_MODEL_6605c07e889a4f43ae9eeb5ffefda3ce","value":" 289/289 [00:00&lt;00:00, 4.08kB/s]"}},"bffa46cb140244769aba92514266cc27":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1735af002c9445bc853ca445a64bdf93","max":210,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8b719707b2e041d68e2412e7e16ad0b7","value":210}},"c05cc1eca0ec426ea8dc95687ce76753":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c904790f16884b1c8edcaab61e570d52":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ca09064a38ba46449fc04f8edf0704e0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d0ac2c4fdfa848309905cb00c01ffe9d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d745ebd64ef04be3aa949a4483442ec0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d98a7c916e9f4886af9495a1e897d1c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f961b7e26c3742258c44298780f98b0a","placeholder":"​","style":"IPY_MODEL_952630f8838b43e2b2e09e3ac29ee5cf","value":"100%"}},"dd836cf6ac984e82aa592f7f8025be6a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e19e8215b7984a50b71f37daa400df05":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e64b028eb3574458a0943246d1699718":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea95640963074b109996160f05439ed5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb9ad837d762489185ecac1528252656":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad4d523ca6b74d32ab46f86f29a447f7","placeholder":"​","style":"IPY_MODEL_ca09064a38ba46449fc04f8edf0704e0","value":"Downloading: 100%"}},"f3c122343e2f49abbdbca22f2dd424cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_275a852cb1984eac927778bf47ee5ac2","placeholder":"​","style":"IPY_MODEL_d0ac2c4fdfa848309905cb00c01ffe9d","value":"Downloading: 100%"}},"f918ed5af5e446e9b5883815169d3184":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f961b7e26c3742258c44298780f98b0a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}